{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716e787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df3554f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of Pride and prejudice, by Jane Austen This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using th'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"Pride and Prejudice.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    "\n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    "\n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8610e97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733851"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f05923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 182, 164, 1001, 3, 299, 4, 946, 30, 72, 710, 41, 1001, 23, 21]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a931558e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131237"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d0f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7250\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ab134d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  131234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[   1,  182,  164, 1001],\n",
       "       [ 182,  164, 1001,    3],\n",
       "       [ 164, 1001,    3,  299],\n",
       "       [1001,    3,  299,    4],\n",
       "       [   3,  299,    4,  946],\n",
       "       [ 299,    4,  946,   30],\n",
       "       [   4,  946,   30,   72],\n",
       "       [ 946,   30,   72,  710],\n",
       "       [  30,   72,  710,   41],\n",
       "       [  72,  710,   41, 1001]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e43e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a5a31bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[   1  182  164]\n",
      " [ 182  164 1001]\n",
      " [ 164 1001    3]\n",
      " [1001    3  299]\n",
      " [   3  299    4]\n",
      " [ 299    4  946]\n",
      " [   4  946   30]\n",
      " [ 946   30   72]\n",
      " [  30   72  710]\n",
      " [  72  710   41]]\n",
      "Response:  [1001    3  299    4  946   30   72  710   41 1001]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed2b804f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ccecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1cf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=25, batch_size=64, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a18079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  sequence = np.array(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "  \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "  \n",
    "  print(predicted_word)\n",
    "  return predicted_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d68471",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "  \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "  \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "        \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "          \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e0481d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
